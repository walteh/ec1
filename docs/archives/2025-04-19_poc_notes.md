EC1: A Weekend POC Roadmap for a Lightweight CloudStack Alternative in Go

Introduction

EC1 is envisioned as a minimal, Go-based alternative to Apache CloudStack – providing VM and network orchestration without the legacy bloat. Apache CloudStack itself uses a heavy Java management server with agents on each hypervisor host ￼. In contrast, EC1 will use two simple Go services (Management Server and Agent) communicating over ConnectRPC (a gRPC + Protocol Buffers framework) for a lean cloud management experience. The goal of this weekend proof-of-concept (POC) is to demonstrate that we can manage virtual machines (including nested VMs and networking) with minimal complexity using modern Go libraries. We’ll show that a single Go-based management server can orchestrate VMs across hosts (macOS and Linux) – all without the traditional complexity of a full CloudStack deployment.

Key POC Outcome: On a macOS host running the EC1 management server, we will boot a Linux virtual machine using Apple’s Virtualization.framework. Inside that Linux VM (running an EC1 agent with KVM), we will launch a nested VM via KVM. The nested VM will run a tiny web server that returns “hello world,” proving that networking and multi-level orchestration work end-to-end.

Architecture Overview

EC1’s architecture closely follows the CloudStack pattern of a Management Server plus per-host Agent, but implemented in Go for simplicity. All internal communication uses ConnectRPC (Buf’s Connect – a gRPC-compatible RPC framework) for efficiency and type-safe APIs ￼. Below is an overview of the components and their interactions:

Architecture of EC1 POC: A macOS host (Level 0) runs the EC1 Management Server and a local Agent. The Mac Agent uses Virtualization.framework to launch a Linux VM (Level 1), which runs its own EC1 Agent. The Management Server communicates via ConnectRPC to both agents. The Linux Agent uses KVM to launch a nested VM (Level 2) that runs a “hello world” web server, demonstrating nested virtualization and networking.
• EC1 Management Server (Control Plane): Orchestrates the lifecycle of VMs across all available hosts. It does not directly interact with hypervisors; instead, it issues RPC calls to the Agents on each host to perform actions (start VM, stop VM, etc.). The management server maintains a view of the available hosts (for the POC, one macOS host and one Linux host) and decides where to launch VMs. In a full system it would handle scheduling, resource tracking, etc., but in the POC this can be simple and static.
• EC1 Agent (Data Plane): Runs on each hypervisor host to carry out commands from the management server. The agent interfaces with the local hypervisor APIs to create, destroy, and manage VMs. In EC1, the Agent is a lightweight Go service exposing an AgentService over ConnectRPC (similar to a gRPC service). The Management Server calls methods on this service (like StartVM) to perform operations on that host. Each Agent is responsible for one host – in our POC, one agent runs on macOS and another runs inside the Linux VM.

ConnectRPC Communication: EC1 uses ConnectRPC for all internal comms. ConnectRPC (from the Buf team) generates idiomatic Go clients/servers from a proto definition, handling the gRPC transport under the hood ￼. This means we define, for example, a StartVM(request) returns (response) RPC in a .proto file, and ConnectRPC gives us a Go interface to implement on the agent side, plus a client that the management server can call. This yields a clean separation: the management server doesn’t need to know hypervisor details – it just calls an RPC, and the agent implementation does the rest. (ConnectRPC is chosen for its simplicity and compatibility with standard gRPC tools ￼, making our stack modern and easy to extend.)

Why this design? It mimics CloudStack’s proven two-tier architecture (central brain with host-specific workers) ￼ but eliminates CloudStack’s Java stack and multiple ancillary services. With everything in Go, we get a single binary for the manager and agent, and communication is efficient (binary protobufs over gRPC) with minimal overhead. This sets the stage for a more developer-friendly cloud management experience – no heavy app servers or XML APIs, just simple Go services.

POC Scenario and Objectives

This POC will be achieved in a series of steps that demonstrate nested virtualization and networking across macOS and Linux hosts. Over a single weekend, the plan is to accomplish the following objectives:
• Target Platforms: Use a macOS host (Apple laptop or similar) as the primary machine, and Linux (Ubuntu or similar) as the guest OS inside a VM. The macOS host supports virtualization through Apple’s frameworks, and the Linux VM will use KVM for nested virtualization. We aim to show EC1 working on both major host types (Unix-based), proving portability.
• Boot a Linux VM on macOS: From the EC1 Management Server running on macOS, launch a Level 1 VM that runs Linux. We’ll leverage Apple’s Virtualization.framework to do this. Apple’s Virtualization.framework provides high-level APIs to run macOS or Linux VMs on modern Macs ￼. Through either a Go binding (like Code-Hex’s vz library) or the HyperKit tool, the macOS Agent will start a Linux VM instance. This first VM will essentially act as a virtual Linux host for further nesting.
• Run an EC1 Agent inside the Linux VM: The Linux VM (Level 1 guest) will boot into a minimal Linux distribution (e.g., Ubuntu cloud image). Inside this VM, we will run the EC1 Agent (compiled for Linux/amd64) to manage that environment. This agent will be nearly identical to the macOS agent in functionality, but it interfaces with KVM. The Management Server on macOS will connect to this Linux agent via ConnectRPC over the network (the Linux VM will be network-accessible to the Mac host). At this point, our EC1 Management Server is orchestrating two agents: one on the Mac itself (which created the Linux VM) and one on the Linux VM.
• Launch a Nested VM via KVM inside the Linux VM: Using the EC1 Management Server (on Mac), we will instruct the Linux Agent to create a Level 2 nested VM using KVM. In practice, the management server might have a notion of “host resources” and choose the Linux VM host for this nested guest. The Linux Agent will then invoke KVM (likely via QEMU) to start another VM inside the Linux VM. This nested VM could be very small (even a lightweight Linux or unikernel). The key is that we are leveraging nested virtualization: a VM running inside a VM. (Note: On many setups, nested virtualization needs to be enabled explicitly. KVM, for example, disables nesting by default and requires a module parameter to enable it ￼. We’ll ensure the Mac’s virtualization allows it – on Intel Macs, the Hypervisor.framework can pass through VT-x, and on Apple Silicon, nested virt is supported on M3 chips as of macOS 15 ￼. For the POC, we assume the hardware/OS supports nested VMs or use an Intel Mac host.)
• Networking & “Hello World” Test: The innermost VM will run a simple HTTP server that returns “hello world” on a test endpoint. This is how we prove that the nested VM is running correctly and is reachable through the network layers. The plan is to configure networking such that the Mac host can ultimately reach the web server in the nested VM (details in the Hypervisor section below). For example, we might forward a port from the nested VM out to the Linux host, and from the Linux host to the Mac, so that a curl on macOS can get the “hello world” response. Alternatively, we can SSH into the Linux VM and curl the nested VM’s service from there as a proof. In any case, seeing the “hello world” output from the nested guest confirms that our orchestration and virtual networking are functioning.

To summarize the flow, on Day 2 of the POC, we expect to: 1. Start EC1 Manager and Mac Agent on macOS. 2. EC1 Manager instructs Mac Agent: “Create VM (Linux)” –> Mac Agent uses Virtualization.framework to boot Linux VM. 3. Linux VM boots up; inside it, we launch the EC1 Linux Agent (manually or via cloud-init). The Linux Agent registers or is known to the Manager. 4. EC1 Manager instructs Linux Agent: “Create VM (nested)” –> Linux Agent uses KVM to boot a nested VM. 5. Nested VM boots and runs a tiny web server (e.g., on port 80). 6. From macOS (or via the Manager), access the nested VM’s web server (through appropriate network forwarding) and receive “hello world”, confirming success.

By achieving these steps, we hit all the POC goals: multi-host orchestration, nested virtualization, networking, and using nothing but Go and system libraries on each platform.

Hypervisor Interfaces and Implementation Details

A core challenge is interfacing with the hypervisors on each platform (macOS and Linux) from Go. We choose the simplest available methods: Apple’s Virtualization.framework on macOS, and KVM (via QEMU/Libvirt) on Linux.
• macOS Hypervisor (Virtualization.framework via Go): Modern macOS (both Intel and Apple Silicon) includes the Virtualization.framework, which allows developers to run VMs with a high-level API ￼. We will use Go bindings for this framework to start our Linux VM. One convenient option is the open-source Code-Hex/vz Go package, which wraps Virtualization.framework calls in Go. Using vz, we can configure a virtual machine with a Linux kernel and disk image, set memory/CPU, and launch it with a few API calls (e.g., vz.NewVirtualMachine(config) and vm.Start() as in the library’s examples). This saves us from writing C/Objective-C code. Another option is HyperKit, a lightweight hypervisor toolkit for macOS ￼. HyperKit is based on xhyve/bhyve and uses Hypervisor.framework under the hood ￼. We could invoke HyperKit as a binary to run a Linux VM (Docker for Mac historically used this). For the POC, using vz (Go bindings) is likely faster to implement, but HyperKit is a fallback if needed (since it’s a ready-made binary). The Mac Agent will encapsulate the logic to start/stop a VM via these interfaces. For example, the Agent’s StartVM RPC handler on macOS might do:

// Pseudo-code for Mac Agent using Code-Hex/vz to start a Linux VM
func (a *AgentServer) StartVM(ctx context.Context, req *StartVMRequest) (\*StartVMResponse, error) {
// Prepare VM configuration (memory, disk image path, network interface)
config, err := vz.NewVirtualMachineConfig(opts...)
if err != nil { return nil, err }
vm, err := vz.NewVirtualMachine(config)
if err != nil { return nil, err }
if err := vm.Start(); err != nil {
return nil, err // return failure if VM couldn’t start
}
return &StartVMResponse{Success: true, VmId: "linux-vm-1"}, nil
}

Virtualization.framework supports standard Virtio devices (network, disk, console, etc.) ￼. For networking, we can use the framework’s built-in NAT interface so the Linux VM gets virtual network connectivity. By default, the VM will likely be behind a NAT that allows it to reach the host (and internet) and the host to reach the VM on a specific IP (often the host can ping the VM’s IP or there may be a VMNet interface). If needed, we can also set up port forwarding on the macOS side (Virtualization.framework allows configuring network devices, and HyperKit has options like -L forward:tcp:...). Since this is a POC, we can accept the default NAT and just ensure we know the VM’s IP (perhaps via DHCP logs or assigning a static IP to the VM in a known range). The Mac agent can then report the new VM’s IP back to the management server if needed.

    •	Linux Hypervisor (KVM via Libvirt/Go): Inside the Linux VM, we have a full Linux OS with KVM capabilities. KVM (Kernel-based Virtual Machine) provides Linux with hardware virtualization support (accessed via /dev/kvm). Typically, one uses QEMU in user-space along with KVM to launch VMs. To control KVM/QEMU from Go, the simplest route is to use libvirt via its Go API. The libvirt project provides a C library to manage VMs (across KVM, Xen, etc.), and we have Go bindings available. We have two main choices: the official libvirt-go bindings (CGO-based) ￼, or a pure Go library like DigitalOcean’s go-libvirt which speaks libvirt’s socket protocol directly ￼. Using libvirt is advantageous because it can handle networking (via NAT or bridged networking setup), storage, and VM lifecycle for us. For this POC, we can run a libvirt daemon inside the Linux VM and have our agent connect to it, or even run QEMU directly without a persistent daemon.

Option 1: Libvirt-go approach: The Linux Agent’s StartVM RPC handler could use libvirt-go to define and start a VM domain. For example:

import libvirt "libvirt.org/libvirt-go"
...
conn, err := libvirt.NewConnect("qemu:///system")
if err != nil { return error... }
// Define domain XML for nested VM (or use an existing XML template)
dom, err := conn.DomainCreateXML(nestedVmXML, 0) // 0 = default flags
if err != nil { ... }
defer dom.Free()
// If needed, get its IP or wait for boot...

This code (structured similarly to libvirt examples) connects to the system libvirt daemon ￼ and creates a new domain from an XML definition. The XML would describe a simple VM: maybe 1 vCPU, 256MB RAM, a virtio disk pointing to an image file (for the nested VM’s OS), and a virtio network interface. We can keep the domain XML minimal. Libvirt will launch QEMU/KVM under the hood. The Agent can then return success (and maybe the new VM’s ID or name). If libvirt is too heavy to set up in the limited time, we can simplify further by invoking QEMU directly.
Option 2: Direct QEMU invocation: Without libvirt, the Linux Agent can execute qemu-system-x86_64 with appropriate flags to start a VM. For instance, using Go’s os/exec to run a command like:

qemu-system-x86_64 -enable-kvm -m 128 -cpu host \
 -drive file=nestvm.qcow2,if=virtio \
 -nic user,hostfwd=tcp::8080-:80

This command would start a nested VM using KVM (-enable-kvm), allocate 128MB of RAM, use the host CPU model (important for nesting), attach a disk image nestvm.qcow2 on a virtio interface, and create a user-mode network interface that forwards the nested VM’s port 80 to port 8080 on the Linux host. The Agent could run this in the background and immediately return success. The hostfwd part is crucial for networking: it means that inside the Linux VM (Level 1), one can access the nested VM’s web server by curling localhost:8080. This makes testing easier – we could SSH into the Linux VM or have the agent itself test the connection.
Nested Virtualization Considerations: We must ensure KVM nesting is enabled on the Linux VM. That means the Mac’s hypervisor needs to expose virtualization extensions to the Linux guest. On an Intel Mac, we’d want the VMX bit exposed (Hypervisor.framework by default may allow it since Docker and others have done nested virtualization via HyperKit). On Apple Silicon, as of 2025, macOS 15+ and M3 chips support nested virtualization ￼, which would allow our scenario. If using older hardware without nested virt, an alternative POC path is to run the Linux “agent” on a physical Linux box or on the Mac itself via another method – but assuming we have support, we proceed with true nesting. We’ll also load the KVM module with nesting enabled: e.g., modprobe kvm-intel nested=1 on the Mac host prior to launching the Linux VM (for an Intel host, as noted in KVM docs ￼). This ensures /dev/kvm inside the Linux VM will function.

    •	Networking Strategy: The final “hello world” must be reachable to demonstrate networking. We have multiple layers of NAT/virt: the nested VM is under the Linux VM, which is under the Mac host. Our approach: use user-mode networking with explicit port forwards at each layer. On the nested VM launch (inside Linux), we forwarded port 80 to Linux’s port 8080 (as in the QEMU example above). Now, we need the Linux VM’s port 8080 accessible from macOS. The Virtualization.framework on macOS by default might give the Linux VM an IP (e.g., 192.168.64.x if using shared networking). We can simply have the Mac host connect to the Linux VM’s IP at port 8080. For example, if the Linux VM gets IP 192.168.64.10 on a virtual interface, and we ran QEMU with hostfwd to 8080, then from macOS we can do curl http://192.168.64.10:8080 and expect “hello world”. If the Mac’s virtualization setup doesn’t allow direct IP connectivity, we could configure a port forward on the Mac agent side as well (some virtualization frameworks allow forwarding a host port into the VM). But assuming default NAT, the host can usually reach the guest via the vmnet gateway IP. We will verify this during the POC: one quick way is to run a simple HTTP server in the Linux VM itself and see if Mac can curl it – if yes, our nested forwarding will likely work too.

In summary, the network flow will be: Nested VM (port 80) → forwarded to Linux VM (port 8080) → reachable by Mac host (via Linux VM’s IP). We’ll document the exact IP/port in the POC instructions so testers know how to see the “hello world”.
• ConnectRPC Setup: We will define the protobuf schema for EC1’s RPCs. Likely we have an AgentService with methods like StartVM(StartVMRequest), StopVM, maybe GetStatus. For the POC we might only implement StartVM and a basic Ping or health check. Using Buf’s tools, we’ll generate Go code for the service interface and clients. The management server will act as a client to each agent’s AgentService. The agents will each run a server (listening on a TCP port, say 8081 on Mac, 8081 on Linux VM) serving the RPC endpoints. When the Mac agent starts the Linux VM, we might have the Linux agent’s address (if we use DHCP, it could register back to the manager). For simplicity, we can have the Linux agent connect out to the manager to register itself (since the manager knows when it asked Mac to create the VM). Alternatively, the manager can poll or try a known IP. Registration can be as simple as the Linux agent calling a RegisterAgent(hostname, IP) RPC on the manager once it’s up (not fully secure or robust, but fine for POC). We won’t implement a full service discovery due to time – static configuration or a simple handshake is enough. No authentication will be in place (the RPC can run insecure on a private network), but we’ll design the abstractions such that adding mTLS or tokens later is straightforward (e.g., ConnectRPC easily integrates with interceptors for auth).

Development Roadmap (48-Hour Plan)

We break the work into two days (assuming a Saturday/Sunday sprint). The focus is on getting basic functionality running first, then iterating to add the nested VM and polish the demo. Below is a detailed roadmap:

Day 1: Scaffold EC1 Services and Launch the First VM (Linux on macOS)

Morning – Project Setup and Architecture:
• Repository Initialization: Create a new GitHub repository (e.g., ec1-poc). Initialize a Go module (e.g., ec1 module path). Establish a clear directory structure (see “Repo Structure” below). Add a README stub outlining the project goal.
• Define Protobuf Schema: Write an ec1.proto file describing the AgentService and perhaps basic messages. For example:

syntax = "proto3";
package ec1;
service AgentService {
rpc StartVM(StartVMRequest) returns (StartVMResponse);
rpc StopVM(StopVMRequest) returns (StopVMResponse);
rpc Ping(Empty) returns (Pong);
}
message StartVMRequest { string image; int32 memory_mb; int32 cpus; /_ etc _/ }
message StartVMResponse { bool success; string vm_id; string info; }
// ... define StopVMRequest/Response similarly, plus Empty/Pong for health checks.

We keep the messages minimal – just enough to specify an image or VM parameters. (We can hardcode values in the agent for POC if needed, but having them in the RPC allows some flexibility.) Use Buf or protoc to generate Go code for gRPC and Connect. This will produce an ec1connect Go package with client and server interfaces.

    •	EC1 Management Server Skeleton: Start implementing the manager in cmd/ec1-manager/main.go. The manager will need to do two things in POC: (1) start its own ConnectRPC server (if we have any manager-side RPCs, e.g., for agents to register), and (2) act as a client to call Agents. We might skip having the manager be a server at all and simply have it orchestrate via client calls (i.e., manager is effectively a CLI tool that invokes agent RPCs in sequence for the demo). To keep it simple, we can design the manager as a command-line program that reads some config (like a list of agent endpoints or actions to perform) and then uses the generated client stubs to call those agent RPCs in order. This avoids needing a manager RPC interface for now.

For example, manager main could: 1. Dial the Mac agent’s RPC endpoint (localhost:8081). 2. Call StartVM to create the Linux VM. 3. Wait/sleep for a bit or poll until the Linux agent is available. 4. Dial the Linux agent’s RPC (IP obtained or predefined, e.g., 192.168.64.10:8081). 5. Call StartVM on Linux agent to create nested VM. 6. Print or log instructions to test the “hello world”.
In code, using ConnectRPC generated client, it looks like:

client := ec1connect.NewAgentServiceClient(http.DefaultClient, baseURL) // baseURL like "http://host:port/" for Connect
// If using gRPC-Go directly, it'd be grpc.Dial and NewAgentServiceClient from protoc-gen-go-grpc.
resp, err := client.StartVM(ctx, &ec1.StartVMRequest{ Image: "ubuntu.qcow2", MemoryMb: 1024, Cpus: 1 })
if err != nil { log.Fatal(err) }
log.Println("Started Linux VM, id:", resp.VmId)

We’ll use Connect’s HTTP/1.1 compatibility to avoid needing HTTP/2 explicitly (ConnectRPC makes it easy to call over standard net/http).

    •	EC1 Agent (Mac) Implementation: In cmd/ec1-agent/main.go, implement the agent service for macOS. We will likely structure it so that the agent can run in either “Mac mode” or “Linux mode,” or simply detect the OS and use the appropriate hypervisor backend. For clarity, we might build two binaries (or one binary with a flag) – but one binary that can handle both by abstracting hypervisor calls is ideal. Create an interface in pkg/hypervisor like HypervisorDriver with method StartVM(image string, memMB int, cpus int) (string, error). Then have two implementations: hypervisor.AppleHV and hypervisor.KVM. The Mac agent will use the AppleHV driver.

Implement the AppleHV driver using Code-Hex/vz (bindings to Virtualization.framework):
• Use the Virtualization.framework to configure a VM. This involves specifying a Linux kernel or boot loader, a disk image, memory, CPU count, and devices (network, console). For the POC, we can simplify by using Apple’s ability to boot a Linux kernel directly with an initrd and command-line – or use an .ISO installer if needed. However, to save time, we might prepare a bootable disk image (say an Ubuntu Cloud image) and use that. The vz library allows loading a Linux kernel image (vmlinuz) and initial RAM disk, but it also supports EFI boot for fully installed OS images. If we have a ready qcow2 or raw disk with Linux installed, we can attach it and boot via UEFI. We will likely use a known-good approach: perhaps we run an Ubuntu cloud image with a cloud-init config to auto-start our agent.
• For now, assume we have linux.img ready. The code would create a VirtualMachineConfiguration with memory, CPU, etc., attach a VirtioBlockDevice for the disk, a VirtioNetworkDevice for networking, and then call NewVirtualMachine(config) and Start(). We’ll wrap this in our StartVM RPC handler.
• If any error occurs (missing entitlements, etc.), note them but continue development – we’ll test on Day 1 afternoon. Ensure we run the agent binary with the necessary rights (Apple’s virtualization may require certain entitlements; if running from terminal on macOS, it should be fine for development usage as Apple allows developer ID to use virtualization).
• Test 1: Launch Linux VM on Mac manually – by mid-day, try running the Mac Agent as a standalone program (without manager) to see if it can start a VM. This might be done by temporarily adding a main in agent to call the StartVM logic directly. We need to verify that a Linux VM boots. If using vz, we might see the VM console in our terminal or attach a console device. If using HyperKit, we might rely on its output. At minimum, ensure the process doesn’t error out. This step is critical to iron out any issues with Virtualization.framework (like missing permissions or incorrect config). Adjust as needed (e.g., if we can’t get a kernel booting in time, use an ISO to boot a live CD). Once we have confidence that the Mac Agent can launch a Linux guest, proceed.

Afternoon – Complete Mac-to-Linux flow:
• Integrate Manager and Mac Agent: Now run the real flow: Start the Mac Agent (listening on, say, localhost:9090 for RPC). Start the Manager which calls the Mac Agent’s StartVM RPC. The Mac Agent should launch the Linux VM. We will likely run the Manager and Agent in separate terminal windows for now. When the Manager calls StartVM, have it block until it gets a response. The Mac Agent, after starting the VM, returns success. The Manager then logs that the Linux VM started. At this point, the Linux VM is running but idle (no EC1 agent in it yet).
• Prepare the Linux VM environment: We need the EC1 Agent binary inside the Linux VM to continue. Easiest approach: include the EC1 Agent binary on the disk image ahead of time. For example, we could mount the image on the Mac and copy the agent binary into /root or set it to run via rc.local. Another quick method: if the Linux VM has an SSH server and we know the credentials (cloud images often default to key auth; we might configure cloud-init with a default password or key for convenience), we can SSH in and manually start the agent. Since this is a one-off demo, a manual step is acceptable. Alternatively, if using cloud-init, provide user-data that fetches the agent binary from the Mac (maybe the Mac can host it via Python HTTP and cloud-init wget’s it). Given the time constraints, manual start is fine: log into the VM (the Virtualization.framework can provide a console or we use vz to attach a pseudo-console device, or we set up SSH via the VM’s IP). Then run the agent (./ec1-agent) inside the VM. It will listen on Linux VM’s 0.0.0.0:9090 for RPC by default.
• Connect Manager to Linux Agent: Once the Linux agent is running, the manager needs to know how to reach it. If we configured the Linux VM networking properly, the Mac host can reach it on an IP (e.g., 192.168.x.x). We use that IP in the manager (we might hard-code it if known, or have the Mac agent print it as part of StartVMResponse). A simple trick: the Mac Agent, after starting the VM, could retrieve the network configuration of the VM. Virtualization.framework might allow getting the NAT assigned IP or we could assign a static IP via config. If not, we can find out via the VM console (e.g., the VM could print its IP via cloud-init once it gets DHCP). Let’s assume we determine the Linux VM’s IP is 192.168.64.10. The Manager can then instantiate an AgentService client for http://192.168.64.10:9090 and call StartVM on it.
• Implement Linux Agent’s StartVM: Now focus on the Linux Agent code (which we actually already wrote the structure for, but now implement with KVM). We decide whether to integrate libvirt or call QEMU directly. Considering the time, calling QEMU via exec might be the fastest path to a result. We will do that for POC (and maybe note that in the future we’d use libvirt for more control). Implement the Linux StartVM handler to execute a QEMU command that starts a nested VM with the “hello world” server:
• Prepare a disk image for the nested VM (could be as small as a few MB). This disk could contain a minimal Linux with a web server that auto-starts. For speed, we might use a pre-built image like Alpine Linux or Tiny Core and write a small init script. Even easier: use a container image with a static binary web server? But we want a full VM, so stick to a tiny Linux.
• Suppose we create an image nested.img that, on boot, runs a script listening on port 80 and responding “hello world”. We place this image somewhere accessible to the Linux agent (maybe we attach a second disk to the Linux VM containing it, or we baked it into the Linux VM image and the agent knows the path). For simplicity, assume /srv/nested.img inside the Linux VM.
• Use os/exec.Command in Go to start qemu: e.g. qemu-system-x86_64 -enable-kvm -cpu host -m 64 -nographic -drive file=/srv/nested.img,if=virtio -nic user,hostfwd=tcp::8080-:80. The -nographic makes QEMU not open a GUI (so it runs in the background).
• Start the process and immediately return StartVMResponse{Success: true, VmId: "nested-1"}. (Optionally, keep track of the process if we want to allow StopVM later, but not required for POC.)
• Test 2: Nested VM Launch – Trigger the manager to call Linux agent’s StartVM. This should spawn the nested QEMU process inside the Linux VM. Because we used hostfwd, the nested VM’s web server should now be listening on the Linux VM’s port 8080. We can verify this in a couple of ways:
• Easiest: within the Linux VM (where we have a shell open for the agent), run curl http://localhost:8080. We should see “hello world” in the output. If yes, the nested VM is running and serving correctly.
• Next, from the macOS host, run curl http://192.168.64.10:8080 (using the Linux VM’s IP and forwarded port). If the networking is correctly bridged/NATted, this should also reach the server. If it doesn’t, we may need to adjust the Mac’s networking (for example, ensure the Mac’s vmnet interface allows incoming connections or use vz networking that supports VM-to-host). We might fall back to just checking from inside the Linux VM for the POC demonstration, but ideally we get it from macOS to truly simulate an external client hitting the nested VM.
• Any issues here (like connection refused) would need troubleshooting: e.g., if using Virtualization.framework NAT, sometimes the host can’t directly initiate to guest (though often it can). If that’s the case, as a workaround, we could have the Linux agent simply print the “hello world” it received as proof (not as nice, but a fallback). However, since the objective is explicit about networking, we’ll aim to get the real network path working. We can also try using VMnet bridged mode on macOS if available, so the Linux VM is on the same LAN as the Mac (with a proper IP). That would make connectivity trivial. But NAT should suffice with correct routing.

By end of Day 1, we should have: the core RPC mechanism working, Mac agent starting a Linux VM, Linux agent (manually started) able to launch a nested VM, and a positive “hello world” test from at least inside the Linux VM. This proves the concept.

Day 2: Automation, Polish, and Documentation

Morning – Automating and Refining:
• Automate Agent Startup in Linux VM: To reduce manual steps, spend some time automating the Linux agent launch. Two possible quick solutions: (1) Use cloud-init in the Linux VM image to run a startup script that executes the agent. For example, embed a cloud-init config that on first boot runs wget http://host/agent-linux -O /root/agent && chmod +x /root/agent && /root/agent > /root/agent.log 2>&1 &. We can host agent-linux binary on the Mac (which the VM can fetch if internet/NAT is working). (2) Alternatively, use an ISO with a startup script or simply document the manual step but present it as a part of running the demo. Given time constraints, documenting the manual start in the README is acceptable for a POC. We will note: “After the Linux VM boots, start the EC1 agent inside it (e.g., login and run ec1-agent &).”
• Implement a Basic Registration (optional): If time permits, implement a simple mechanism for the Linux agent to notify the manager that it’s alive. This could be as simple as the Linux agent making an HTTP request to the manager or printing a message that the manager reads via console. Since our manager sequence is predetermined, we might skip this and simply put a time.Sleep(30 \* time.Second) after creating the Linux VM to give the user time to start the agent and for the VM to come up, then proceed to nested VM launch. (Not elegant, but workable in a scripted demo.)
• Add StopVM or Cleanup (optional): We can add the ability to stop VMs just to demonstrate completeness. For Mac agent, that would call vm.RequestStop() or kill the process; for Linux agent, kill the QEMU process or use libvirt Destroy. However, for a quick POC, we might skip actual Stop implementation to save time, or implement it only for the Mac side (to stop the Linux VM at the end of the demo). Mark this as a potential extension.
• Finalize Networking Setup: Double-check how the “hello world” can be retrieved. If direct curl from Mac to nested doesn’t work, consider adding a port forward on Mac side: Virtualization.framework’s network config might allow a MAC to be assigned or we might do SSH port forwarding from Mac to Linux VM. Another trick: run a small proxy in the Mac agent that listens on a port and tunnels to the Linux VM’s 8080. But this complexity might not be needed if the default vmnet works. We’ll test and document whichever method we use to get the output.

Afternoon – Testing End-to-End and Documentation:
• Full End-to-End Test: Run through the entire sequence as if you were a user following the README: 1. Start ec1-agent on Mac. 2. Start ec1-manager on Mac. 3. Manager triggers Linux VM -> verify Linux VM boots. 4. (Manual step: start agent in Linux VM, if not automated.) 5. Manager triggers nested VM -> verify “hello world” accessible. 6. Use manager or manual steps to shut down VMs if needed.
Ensure each step works reliably or adjust timing (e.g., add waits or retries around VM startup, since booting an OS can take some time). We can implement a simple polling in the manager – e.g., after starting Linux VM, try to ping the Linux agent’s port until it’s up, then proceed.
• Code Cleanup: Organize code into proper packages, add comments, handle errors gracefully (at least log them). Ensure that configuration (paths to images, etc.) are not hardcoded or are easy to change (maybe via flags or env variables). For example, the Mac agent can take --disk-image=path and the Linux agent --nested-image=path so others can replicate with their images.
• Documentation (README): Write a comprehensive README.md that includes:
• Project Overview: What is EC1, what does this POC do, and why (mention goals like avoiding Java and being developer-friendly).
• Architecture: Describe the two services and how they interact, similar to the content above. Include the architecture diagram for clarity. The diagram helps newcomers quickly grasp the setup.
• Setup Instructions: Detailed steps to run the POC: 1. Requirements: macOS with Go 1.xx, virtualization enabled; an internet connection (if needed to download images); possibly Homebrew (if they need to install QEMU inside the Linux VM or we bundle everything). 2. How to build the binaries: e.g., go build ./cmd/ec1-manager and go build ./cmd/ec1-agent (for Mac, and cross-compile for Linux agent if needed). 3. How to prepare the VM images: Provide links or instructions for obtaining a Linux image for the Mac agent to use (perhaps point to an Ubuntu cloud image download) and preparing the nested VM image. If we can share a prepared image (via the repo or a link), that would be ideal. Otherwise, instruct something like using qemu-img to create a disk and installing a minimal OS or mention that any small Linux kernel+initrd that prints hello is fine. 4. Running the demo: Step-by-step commands. For example:
• Open Terminal 1: ./ec1-agent --mode mac --disk ./ubuntu.qcow2 --grpc-bind :9090 (start Mac agent).
• Open Terminal 2: ./ec1-manager --agent-ip 127.0.0.1:9090 --nested-ip 192.168.64.10:9090 (start manager, which will orchestrate).
• The manager will output logs like “Linux VM started at IP X”, “Nested VM started”, etc.
• How to access hello world: e.g., “Run curl http://192.168.64.10:8080 from your Mac after seeing the ‘nested VM started’ message”.
• Also mention how to shut everything down (maybe instruct to ctrl+c agents or use virsh if applicable).
• Code Samples: Include a few snippets in the README to illustrate how EC1 works under the hood. For instance:
• A snippet of the proto file to show how simple the service definition is (this can excite developers that it’s just a few lines of proto to define a cloud API).
• A short excerpt from the agent code (like how the Mac agent calls Virtualization.framework or how the Linux agent launches QEMU). Showing 5-10 lines of Go can be compelling to readers to illustrate the simplicity (e.g., vz.NewVirtualMachine(config) or libvirt.NewConnect(...) usage).
• POC Scope & Limitations: Clearly state that this is a weekend POC, so things like security (no auth), persistence (no state save if manager restarts), robust error handling, and a user-friendly UI are not implemented. However, also highlight that the core achievement is there: VM orchestration across two levels using pure Go, which is a strong foundation.
• Architecture Diagram & Explanation: Embed the system diagram (similar to what we have above) to visually explain the environment.
• Next Steps (Future Work): A brief section about what could be done if the project continued – for example, adding a web UI or CLI, implementing a scheduler, supporting additional hypervisors (like VMware or Hyper-V) with the same agent interface, etc., and integrating real auth and multi-tenant features. This casts the vision that EC1 could grow into a full CloudStack replacement.
• Project Marketing Prep: Beyond the README, prepare the repository for launch:
• Ensure all source files have a proper license header (likely Apache 2.0 or MIT license chosen).
• Write a CHANGELOG.md or at least tag a version (v0.1.0) for this POC release.
• Create a concise description for the repo (e.g., “EC1 – a minimalist CloudStack-like cloud orchestrator in Go (weekend project)”).
• Possibly create a logo or use an emoji for fun in the README (not mandatory, but “marketing-oriented” could include having a nice presentation).
• Include the usage of modern tech in keywords: Go, gRPC, KVM, Apple Virtualization – to attract interest from those communities.

Evening – Final Checks and Launch:
• Double-check that all citations (if any in documentation) are correct and all references in code/README are consistent (for example, ensure the IP addresses used in examples match what the code does by default).
• Commit and push all code to GitHub.
• Publish the repository and perhaps write a short launch post (could be a GitHub Gist or a Medium article, depending on the target audience). Emphasize the achievement: “From zero to nested virtualization in one weekend, using Go!”. This adds a marketing shine to the technical work.
• Share the project with the relevant communities on Monday: post on Hacker News (“Show HN: EC1 – CloudStack reimagined in Go (POC)”), Reddit r/golang or r/cloudcomputing, and tweet about it with screenshots or GIFs of the console output. This aligns with the marketing checklist to generate interest and gather feedback.

Repository Structure and Code Layout

Organizing the project well is important for clarity. Here’s a suggested layout for the EC1 POC repository:

ec1-poc/
├── cmd/
│ ├── ec1-manager/
│ │ └── main.go # Management server/CLI entry point
│ └── ec1-agent/
│ └── main.go # Agent service entry point
├── pkg/
│ ├── rpc/ # Generated code and RPC helpers
│ │ ├── ec1.proto # Protobuf definitions (source)
│ │ ├── ec1connect.go # ConnectRPC generated client/server stubs (Go)
│ │ └── ec1_grpc.go # (If using grpc-go plugin as well)
│ ├── hypervisor/
│ │ ├── apple_hv.go # Implementation of HypervisorDriver using Apple Virtualization.framework
│ │ └── kvm.go # Implementation of HypervisorDriver using KVM/QEMU
│ └── util/
│ └── network.go # (Optional) helpers for networking, e.g., get VM IP
├── images/
│ ├── linux-vm.qcow2 # (Placeholder or instructions to download) Linux image for Mac agent to boot
│ └── nested-vm.qcow2 # (Placeholder/created) Nested VM image with web server
├── README.md # Detailed instructions and project description
├── ARCHITECTURE.md # (Optional) Separate architecture write-up if needed
└── go.mod / go.sum # Go module files

In this structure, cmd/ec1-manager and cmd/ec1-agent are the two binaries. The pkg/rpc directory holds the proto and generated code (so that both manager and agent can import the definitions). The pkg/hypervisor contains the platform-specific logic for managing VMs; the agent will call into this. This separation makes it easy, for instance, to add a hypervisor/xen.go or others in the future, implementing the same interface. We also include an images/ directory or references, though we might not store large image files in Git – instead we’ll provide scripts or links to get them. For now, it’s a placeholder to remind users what images to prepare.

Code Highlights:
• Protobuf Definition (rpc/ec1.proto) – the contract between manager and agent. It defines messages like StartVMRequest (with fields for image path, resources, etc.). This design allows the EC1 Manager to be fairly abstract – it just knows it can ask an agent to start a VM with X parameters. We might add an AgentStatus message in the future to let agents register their capabilities (e.g., host OS type, available RAM, etc.), but for the weekend POC, hardcoded knowledge is fine.
• Management Server (cmd/ec1-manager/main.go) – acts as a coordinator. It likely won’t run a persistent server in POC, but rather be a sequence of RPC calls. In a more elaborate setup, the manager could maintain a gRPC server for agents to report in or for a UI to connect, but since there’s no UI/API in scope, we keep it simple. The manager uses the AgentServiceClient (from ConnectRPC) to call each agent. For ConnectRPC, we use an HTTP client with the agent’s URL (ConnectRPC can work with plain HTTP/2 or even HTTP/1.1). This means no heavy gRPC server reflection or load balancers needed – just direct calls. The output of the manager will be logged to console for the demo. We can implement it as a sequence of steps, or even interactive CLI (but sequential is easier given time).
• Agent Server (cmd/ec1-agent/main.go) – sets up an HTTP server (or gRPC server) listening on a port for the AgentService. We leverage Connect’s http.Handler for the service, or use grpc-go’s server if we prefer. The agent when started will decide which hypervisor driver to use. Pseudocode:

driver := hypervisor.SelectDriver() // picks AppleHV or KVM based on runtime GOOS, or a flag
svc := &AgentServiceImpl{ driver: driver }
mux := http.NewServeMux()
// using Connect
path, handler := ec1connect.NewAgentServiceHandler(svc)
mux.Handle(path, handler)
http.ListenAndServe(":9090", mux)

This would make the agent listen for incoming RPC calls. (Alternatively, with gRPC server, we’d do grpcServer.Serve(lis) after RegisterAgentServiceServer(grpcServer, svc)). Connect simplifies some of this by providing the handler directly.

    •	Hypervisor Drivers (pkg/hypervisor/.go)* – contain the system-specific calls:
    •	apple_hv.go: Uses the Code-Hex/vz package (which in turn calls Apple’s Virtualization.framework) to start/stop VMs. We’ll include code to set up a Mac VM config. Possibly use a utility function to find an available Linux kernel on the system, or an EFI firmware path, etc. If those are not readily available, using a full disk image with EFI is simplest. The code will check for errors at each step and return them up.
    •	kvm.go: Uses either libvirt or executes QEMU. If using libvirt, it would use the libvirt-go API to define a transient domain from a template XML (we can include a template string in the code for now, filled in with the image path and memory values from the request). If using QEMU directly, it will form the command-line string (as discussed earlier) and run it. We should make sure to run QEMU with -daemonize or run it in a goroutine so that the agent’s RPC handler isn’t blocked forever (since we want to return as soon as VM is launched). We can redirect QEMU output to a log file to keep the agent output clean.
    •	For StopVM, if we implement it, Apple’s framework provides a .Stop() or sending a SIGKILL to hyperkit process, and libvirt provides DomainDestroy or we kill the QEMU process. This is doable but optional for POC.

By structuring the code in this manner, developers can easily navigate: the separation of concerns is clear (RPC vs hypervisor control), and it will be obvious where to extend functionality (e.g., adding new RPCs or supporting another hypervisor technology). We’ll include comments in these files explaining the approach, so anyone reading the code understands, for instance, that “here we call QEMU with nested virtualization enabled” etc., with reference to why certain flags or settings are used.

Deliverables

By the end of the weekend, we will deliver a working codebase in a clean GitHub repository along with documentation and examples. Specifically, the deliverables include:
• Complete EC1 POC Source Code – All Go code for the management server and agent, organized as described. It will be buildable and runnable on macOS (for manager + Mac agent) and on Linux (for the Linux agent, though one might cross-compile that binary from Mac). We will have instructions to compile and set up everything. The code will be relatively small (likely a few hundred lines of Go excluding generated code), focusing on the essential tasks (RPC + starting VMs). Despite being a quick POC, we’ll ensure the code is readable and avoid overly hacky shortcuts so that others can understand it. Key algorithms (like how we launch the VM) will be documented in comments.
• README with Architecture & Setup Guide – As outlined, a step-by-step guide on how to reproduce the POC. This includes how to get the needed VM images (for licensing reasons we might not include OS images in the repo, but we can guide users to download Ubuntu Cloud Image for the Linux VM, and maybe provide a pre-built tiny image for the nested VM). We will also describe the architecture (with a diagram) in the README so readers grasp the design before diving into code.
• Usage Example (Hello World verification) – The README will contain an example run (or we might even include a script run_demo.sh that automates the sequence if possible). It should show, for example, console logs from the manager like:

Manager: Starting Linux VM on Mac agent...
Agent[Mac]: Booting VM with image ubuntu.qcow2 (1 CPU, 1024 MB)...
Agent[Mac]: VM started, IP=192.168.64.10
Manager: Linux VM started (ID linux-vm-1). Waiting for agent...
[Linux agent comes online]
Manager: Starting nested VM on Linux agent...
Agent[Linux]: Launching nested KVM VM (TinyLinux.img)...
Agent[Linux]: Nested VM started (ID nested-vm-1).
Manager: Nested VM started. Test by running `curl http://192.168.64.10:8080` ...

And then perhaps show the output of the curl: "hello world". This textual example will prove that the system works as described. We will of course only include this once we have successfully tested it.

    •	Code Samples in Documentation: To make the repo launch more “marketing-friendly,” we’ll highlight some code in the documentation that shows how simple things are with Go. For instance, a snippet showing how few lines of code it took to start a VM using Apple’s framework, or how the RPC call looks like a normal function call. This will attract Go developers by demystifying hypervisor control. Example to show in README or a blog:

“EC1 uses Apple’s Virtualization.framework via a Go binding – for example, to launch a VM we simply do:

vm, _ := vz.NewVirtualMachine(config)  
_ = vm.Start() // Boot the VM

and on Linux, to launch a KVM VM:

cmd := exec.Command("qemu-system-x86*64", "-enable-kvm", "-m", "64", "-drive", "file=disk.qcow2,if=virtio", ...)  
* = cmd.Start() // Start nested QEMU VM

All orchestrated by high-level Go code with RPC calls connecting the pieces.”

    •	Architecture Diagram and Design Doc: We will include the architecture diagram (as an image file) in the repo, and reference it in the README. Additionally, we might include an ARCHITECTURE.md for those who want a deeper dive into why we chose certain technologies (ConnectRPC, etc.), essentially a write-up of what’s in this answer. This can help when “marketing” the project to an audience that likes to read design rationales.
    •	Basic Launch Checklist Document: A short checklist in the repo (possibly in the README or a CONTRIBUTING.md) that states how to run tests (if any), how to contribute, etc. Not much for a POC, but perhaps how to file issues or contact us.

Everything will be packaged so that someone with a weekend to spare can try out EC1 POC themselves, or a curious CTO can glance at the README and immediately see the value (no complex install, just Go binaries and a quick demo – developer-friendly cloud orchestration).

Marketing-Oriented Repo Launch Checklist

To ensure EC1 POC not only works but also garners attention and is easy to understand, here is a checklist of items we’ll cover when launching the repository:
• ✅ Clear Project Naming and Tagline: The repository will be named clearly (e.g., ec1-poc). The tagline in the description will read something like: “EC1 – A CloudStack-inspired lightweight cloud orchestrator in Go (weekend POC)”. This immediately tells what it is and hints at the speed of development, which can intrigue readers (i.e., “built in a weekend” is impressive).
• ✅ Engaging README: The README will serve as the landing page for the project. We ensure it has:
• A visual diagram at the top (so even at a glance one sees an architecture picture).
• Badges for GoDoc, license, etc., if applicable (these add credibility).
• Introduction and motivation (pointing out the pain of CloudStack’s complexity vs this simple approach).
• Quickstart instructions (so potential users can get it running without digging).
• Technical details for those interested (so it appeals both to high-level readers and low-level implementers).
• A note that it’s a POC and welcome feedback/contributions (creating an inviting tone).
• ✅ License and Contributor Info: We will choose an OSS license (likely Apache 2.0 to align with CloudStack’s license and encourage enterprise adoption). Include a LICENSE file. Also credit any third-party libs (like Code-Hex/vz) in the README.
• ✅ Documentation of Setup: Ensure that all steps to run are documented – including any quirks like enabling virtualization on Mac (System Preferences) or installing QEMU on the Linux VM if we needed to. We want anyone who finds the repo to be able to reproduce the demo without contacting us. Testing those instructions on a clean environment is part of the checklist.
• ✅ Visual Aids: Besides the architecture diagram, consider adding a simple ASCII diagram for the network port forwarding (if needed) or a table listing the layers (Mac host, Linux VM, Nested VM) and what runs where. This helps explain nested virtualization to those unfamiliar. We avoid overly complex diagrams, but a few visuals in a project page make it memorable.
• ✅ Emphasize the “All in Go” aspect: In the project description and announcement, highlight that no Java, no heavy frameworks are involved – just Go binaries and system APIs. This is a major selling point (“goodbye 8GB CloudStack management server, hello 20MB Go binary”). Possibly include a comparison blurb in README: “Apache CloudStack: Java, MySQL, dozens of services. EC1: Go, gRPC, and OS virtualization frameworks – that’s it.” This positioning will attract those who desire simpler cloud management solutions.
• ✅ Announce and Share: Plan to share the project in relevant channels:
• Post on Hacker News and Reddit (maybe r/homelab, r/selfhosted, since those users might love a simple cloud manager). Provide a concise summary and link to the GitHub.
• Tweet a short demo video or GIF (maybe a terminal screencast showing the manager issuing commands and then curling “hello world”). Visual proof can excite potential users.
• If time permits, write a Medium or Dev.to blog post titled “Building a Cloud Orchestrator in a Weekend with Go: The EC1 Story”. This can drive additional interest and explain the concept to a broader audience.
• ✅ Future Plans and Call for Feedback: Make it clear this is just a beginning. Invite cloud developers or operators to give feedback via GitHub issues. Possibly start a GitHub Discussion or use the repo’s issue tracker for design ideas (like “Which feature would you like to see next?”). This not only helps marketing but could guide the next steps if the project continues.
• ✅ Highlight Developer Experience: As part of marketing, underscore how a developer or sysadmin can easily play with EC1. For instance, because it’s all in Go, one could add a new RPC or integrate a new hypervisor quickly. The repository might include a “Developer Guide” section for building from source, modifying protos, etc. This encourages open-source contributions by lowering the barrier to entry (unlike CloudStack where contributing means dealing with a huge Java codebase).

By following this checklist, we aim to not only deliver a functioning POC but also present it in a polished manner that attracts attention and clearly communicates the value proposition of EC1. The ultimate goal of EC1 (even beyond this POC) is to demonstrate a CloudStack-like experience with drastically reduced complexity, and the way we package and market this POC should strongly convey that message.

⸻

With this roadmap and plan, EC1’s weekend POC will conclusively show that a lightweight cloud orchestrator using Go is not only possible but practical. We will have orchestrated a nested “Hello World” cloud entirely via Go code, ConnectRPC, and built-in hypervisors – no legacy baggage included. By Monday, the world can see a GitHub repo with EC1 POC, complete with documentation, diagrams, and a demo, proving out the concept and laying the groundwork for a modern, developer-friendly CloudStack successor.

Sources:
• Apache CloudStack’s traditional architecture (Java management server + agents) for context ￼.
• ConnectRPC by Buf, used for gRPC communication in EC1 ￼.
• Apple’s Virtualization.framework (via Code-Hex/vz) for macOS VM support ￼.
• HyperKit as an alternative lightweight hypervisor on macOS ￼.
• Libvirt and go-libvirt for managing KVM virtual machines via Go ￼ ￼.
• KVM nested virtualization enabling (ensure outer host supports it) ￼.
