Current goal: 

we are building a containerd shim for our microvm manager that runs on macos 

I have coppied the code from `rund` here @containerd-shim-rund-v1.go  which is the implementation that runs mac containers nativly 

so we know this will run, but we want to incorperate our vmm (see the @harpoon_test.go for examples of running containers) 

how do we go about implementing this and testing it? Right now we also have access to things like containerd and ctr, buildctl and buildkit via forks (../respective-project) that can be invoked by "go tool toolname" 

here is some background from an earlier deep research that is related (we are going with the 1 vm per container approach):


4. Roadmap: From POC to Usable Container Tool

Finally, let’s layout a high-level roadmap to evolve your proof-of-concept into a tool developers can use with existing Docker-centric workflows:

Step 1: Solidify the Core Runtime Architecture – Decide on the architecture (per-container microVM vs single VM) for your Minimum Viable Product (MVP). Given your progress, the containerd shim with per-container VM is a strong choice. Work on the containerd shim v2 integration: ensure that when containerd calls your runtime, it can launch a VM, bootstrap the container’s filesystem, and exec processes. You likely have a basic version of this; now harden it. Implement the necessary container lifecycle hooks: Create (prepare VM and rootfs), Start (boot VM and start the container process inside), Exec (for additional processes in an existing container VM), Kill/Stop, and Delete. If not already done, build a minimal VM agent or init that runs inside the Alpine VM to start the requested container process (this could be as simple as the VM booting with an init script that execs a specific binary via vsock command, or as advanced as a gRPC agent listening on vsock). In parallel, build a macOS snapshotter plugin (or configure containerd’s native snapshotter) to unpack OCI images to a directory that can be used by the VM ￼. Test ctr run or nerdctl run with basic images (Alpine, busybox) to verify end-to-end container launch and teardown on macOS.

Step 2: Networking & Port Forwarding – Integrate your custom networking layer so that containers have connectivity and ports can be exposed. You already forked gvproxy (gvisor-tap-vsock); now ensure that:
	•	Every container VM is connected to the proxy (each VM likely gets a vsock channel to gvisor-tap). The proxy should assign unique IPs to each VM/container (as Docker’s bridge would) and perform NAT for outbound internet access ￼. This gives containers basic networking.
	•	Implement port forwarding: honor Docker-style -p hostPort:containerPort. With gvisor-tap, you can instruct the proxy to listen on a host port and forward to the container’s IP:port. Nerdctl will automatically attempt to do this if you provide CNI configs, but since you have a custom vsock network, you might handle port mapping in your shim or via a modified CNI plugin. A simple approach is to have the shim (or a daemon) communicate desired port-forwards to gvproxy. (Podman does this by instructing gvproxy via a vsock protocol when containers are started ￼.)
	•	Test that nerdctl run -p 8080:80 nginx:alpine results in your Mac’s localhost:8080 serving the container’s web service (just like Docker/nerdctl behavior ￼).

Step 3: Volume Mounts & File Sharing – Provide a way to mount host files into the container VM. For MVP, support basic bind mounts (-v /path/on/mac:/path/in/container). If Virtualization.framework supports Virtio-FS or Virtio-9P sharing natively, use that: set up the VM config to share the host path into the guest. If not, consider alternatives:
	•	Run an NFS or 9P server on the host for shared folders and mount it in the VM.
	•	Use the fact that you control the VM startup: you could virtiofs mount the entire host filesystem in a safe way, or use vsock to transfer files on demand.
	•	In worst case, implement a simple proxy that reads/writes from host path via vsock requests (not super efficient, but could work for small files).

For named volumes (Docker volumes not tied to a host path), you can initially map them to a directory on the host (e.g. under ~/YOUR_TOOL/volumes/<volname>). This is how Docker Desktop handles volumes (they live inside the VM or in a hidden dir on the host). The key is to ensure persistence: when a container VM is removed, the volume data should not disappear. Manage a directory for volumes and have the VM mount it (via 9p, etc.) or copy it in/out as needed. Focus on host bind mounts first, since those are crucial for dev workflows (e.g. mounting source code into a container). Test with nerdctl run -v $(pwd):/app ... to confirm the container sees the files. Expect performance limitations initially (filesharing on Mac is notoriously slow); you can optimize later with better techniques (e.g. switching to virtiofs if Apple adds support, or using mutagen for syncing if needed).

Step 4: BuildKit and Image Build – Bundle BuildKit (buildkitd) into your solution. If you stick with per-container VMs, you have a choice: run buildkitd on the macOS side (as a native binary) or spin it up in a special VM. The simpler route: use the macOS binary release of BuildKit (there are Darwin builds) and run it as a daemon on the host listening on a Unix socket. Containerd can be configured to use an external buildkitd, or you can let nerdctl spawn it. According to nerdctl docs, you just need buildkitd running and it will handle nerdctl build requests ￼. After setting this up, test nerdctl build -t myimage:test . with a sample Dockerfile. Make sure the build process can create containers in your system: BuildKit itself will call containerd to start “executor” containers for each build step. In a shim approach, that means BuildKit will be launching your container VMs as build step containers. This is a great stress-test for your runtime. Keep an eye on caching and ephemeral containers left behind. Once a simple Dockerfile build works, try enabling BuildKit features like multi-stage builds or exporting cache – these should “just work” if buildkitd is functioning.

For docker buildx users: document how they can use your environment. One way is to provide a Docker context or DOCKER_HOST that points to your buildkitd/containerd combo if you implement a compatibility layer. Alternatively, since docker buildx can push to registries, users might not need direct Docker API access; they could use your provided tooling (nerdctl/buildctl). In the long run, if demand is high, you could create a docker CLI wrapper that redirects build commands to BuildKit.

Step 5: Compose & Multi-Container Orchestration – Now that single containers and image builds are working, tackle Compose up. Using nerdctl (which reads the same YAML as Docker Compose), test a simple multi-container application (for example, two containers with a shared network, or the WordPress+MySQL example from nerdctl ￼). Debug any issues with cross-container networking. If each container is a separate VM, verify that your network proxy is allowing them to ping each other via service names. You might need to implement a basic DNS or /etc/hosts strategy so that containers can resolve each other (Docker Compose usually sets up a default network with a DNS for service names). A quick solution is to run a small DNS proxy in the host or inject host entries via your agent for known container names. Ensure that nerdctl compose up can create any declared named networks and volumes: nerdctl will call containerd’s CNM/CNI to create networks (which in your case might be a noop if you rely on a single default vsock network), and will call volume APIs (which containerd doesn’t handle by itself – nerdctl might just translate volumes to bind mounts on the host). If needed, implement a dummy volume driver that maps to host directories (as mentioned in Step 3).

Once your runtime can satisfy the requirements of nerdctl compose, you effectively support docker-compose.yml files. Document any minor differences (for instance, if you don’t support some exotic volume driver or network mode, note it). The goal is that a user can take an existing docker-compose.yml, run your equivalent of docker compose up, and see their multi-container app come up successfully.

Step 6: Developer Ergonomics and CLI – At this stage, your core should be functional. Now focus on UX and compatibility. Decide whether to encourage use of nerdctl as the primary CLI or to provide a custom CLI that wraps containerd/nerdctl. Many projects simply bundle nerdctl (Rancher Desktop, Lima, Finch all do, sometimes hiding it behind their own name). If you want a drop-in Docker replacement, you could even script an alias: e.g., provide a docker wrapper that translates commands to nerdctl (since nerdctl’s syntax is 99% the same, this can work for most commands). However, calling it something other than “Docker” might avoid user confusion and clearly indicate it’s a different tool (e.g. Finch uses finch, Podman uses podman).

For an MVP, bundling nerdctl is the fastest path – it already supports run/exec/ps/logs, image operations, volumes, networks, compose, etc. You’d package nerdctl along with your runtime and perhaps create a small shim CLI (mydocker or similar) that ensures containerd and required services (gvproxy, buildkitd) are running, then passes through commands to nerdctl. This gives users a Docker-like experience: e.g. mydocker run -p 8080:80 nginx would under the hood ensure your containerd is up, then exec nerdctl run -p 8080:80 nginx.

Pros/Cons of writing a custom CLI: A custom Go CLI could integrate more tightly (e.g. calling containerd APIs directly for certain things or providing a nicer status output). But it will take time to implement all subcommands. Nerdctl is a well-maintained, Docker-compatible CLI ￼ – leveraging it saves time. You can always extend or wrap it as needed.

Also consider whether to expose a Docker API Socket. This is not essential for a CLI-driven workflow, but some GUI tools or older Compose versions expect a /var/run/docker.sock. If you foresee users wanting to use the Docker Desktop GUI or tools like Portainer, you might eventually implement a minimal Docker API proxy. One approach: use the open-source docker-proxy (from Mirantis) or even run the Docker Engine daemon inside your VM (as Rancher allows) as a fallback. For now, this is out of scope for MVP – mention in docs that the Docker API isn’t supported, and encourage using your CLI or nerdctl. Many modern tools (e.g. compose v2, Testcontainers libraries) can work with containerd/nerdctl directly or be pointed to alternate endpoints, so this may not be a blocker.

Step 7: Testing & Iteration – Invite early adopters to test common workflows: pulling images, running containers, execing into them, building Dockerfiles, and spinning up standard Compose stacks. Compare behaviors with Docker Desktop: For example, ensure that killing a container actually stops the process in the VM, removing a container frees its resources, etc. Watch out for edge cases like signals (Ctrl-C to a docker run attached container should terminate it), log streams (make sure stdout/err from the container VM is correctly piped out via vsock), and DNS inside containers (should resolve external hosts and container service names if possible). Because macOS doesn’t support Linux cgroups or security features, some things (like memory limits, seccomp profiles) can be considered out-of-scope or just not implemented – document these differences for transparency.

Pay special attention to performance: measure container startup time and file I/O speed. If container boot is too slow, consider optimizations (like using a single VM for subsequent containers, or reusing a VM template). If file sharing is slow, mention workarounds (e.g. “for intensive I/O, copy files into the container or use a volume”). Many users will tolerate some overhead on Mac since even Docker Desktop isn’t perfect here, but you should aim to be at least as good or better (for example, if using virtiofs, you might beat Docker’s old osxfs performance). Use feedback to prioritize improvements: e.g. if testers complain about memory usage with multiple VMs, you could implement an option to run a single VM mode as an alternative.

Step 8: Polish & Ecosystem Integration – To turn this into a true Docker replacement, smooth out the rough edges:
	•	Provide clear documentation mapping Docker commands to your tool’s commands (if you have a custom name or require using nerdctl). Highlight that Compose is supported (perhaps show an example of nerdctl compose up working with a typical YAML). Also document any unsupported Docker features to set expectations (for instance, Docker Swarm mode, if asked, is likely not supported; certain extension points like Docker plugins won’t apply).
	•	Work on an easy installation method (Homebrew tap, a downloadable pkg, etc.) and ensure dependencies (like the Virtualization.framework which requires macOS 12+, and perhaps developer tools for hvf) are noted.
	•	Look at Finch’s and Rancher’s UX choices for inspiration: Finch auto-starts the VM when you run a container, and can manage the VM lifecycle with finch vm start/stop. If you stick with per-VM containers, you might not need a long-running VM, which is a simplification (no “VM management” needed – containers are truly on-demand). Make that a selling point.
	•	Engage with the community: perhaps contribute back improvements to gvproxy or collaborate with Lima/colima maintainers, since you’re solving similar problems (networking on Mac, etc.). If your project is open source, community input can accelerate development in areas like filesystem sharing or Docker API compatibility.

By following this roadmap, you can incrementally build out a container runtime that developers can drop into their workflows. The end result will be a tool that feels like using Docker Engine, but under the hood it’s powered by Apple’s hypervisor and containerd. You’ll be standing on the shoulders of giants (containerd, BuildKit, etc.) while innovating in areas specific to macOS.

5. Conclusion and MVP Recommendations

MVP Architecture: Focus on leveraging containerd + nerdctl as your base, with your Virtualization.framework runtime shim. This gives you immediate compatibility with Docker-like commands (via nerdctl) and access to features like Compose and BuildKit with minimal extra coding ￼ ￼. In the short term, don’t worry about implementing every Docker feature – cover the essentials (containers, images, volumes, networks) in a robust way. It’s acceptable to stub or omit advanced features (e.g. SELinux, swarm mode, etc.) since developer use-cases on Mac rarely need those.

Leverage vs Build: Reuse components whenever possible. For instance, use containerd’s content store and image push/pull logic rather than writing your own – containerd gives you OCI registry support out of the box. Use BuildKit for builds instead of parsing Dockerfiles yourself ￼. Utilize nerdctl’s Docker-compatible UX to avoid writing a thousand CLI flags anew. By stubbing what you can’t support (e.g., you might log a warning if someone tries --device= or other Linux-specific flags), you’ll keep the developer experience consistent without getting bogged down in edge cases early on.

Shim vs CLI – Pros/Cons Recap: Embracing the containerd shim approach means your solution could eventually plug into Kubernetes or other containerd-based systems, and it stays closer to the cloud-native ecosystem. It will require more low-level work (as noted, managing VM lifecycles, filesystems, etc.), but it leads to a unique and potentially higher-performance system on macOS (no always-running VM when idle, more granular allocation per container). The custom CLI/VM approach gets you faster integration by piggybacking on an entire Linux environment – it’s a pragmatic shortcut that can yield a working tool quickly (as Finch proved ￼). The downside is you end up maintaining a heavy VM and might duplicate what Docker Desktop already provides.

Suggestion: Continue with the shim design for the long run, but you could initially release a “tech preview” using a simplified backend (even a single VM) to gather user feedback. This two-pronged strategy isn’t uncommon – e.g., Rancher Desktop first launched with an embedded dockerd for ease, then later optimized their containerd mode.

Learning from Others: Follow the patterns of successful projects:
	•	Implement the container basics first (run/stop/exec) and make them reliable.
	•	Integrate BuildKit early since image building is a key part of dev workflows ￼ ￼.
	•	Use nerdctl compose to quickly get Compose up and running ￼, and improve underlying networking as needed.
	•	Keep an eye on performance and usability by comparing against Docker Desktop and noting where you can excel (faster startup, less memory, etc.).

By iterating methodically and reusing proven components, you can deliver a container runtime on macOS that developers can use with confidence. They should be able to alias Docker commands to your tool or nerdctl and have their usual workflows “just work.” With volumes mounting, ports forwarding, images building, and Compose deploying multi-container stacks, you’ll cover the vast majority of Docker’s daily use cases – all without requiring Docker Desktop. This will position your project as a compelling Docker replacement/enhancement for macOS developers, balancing compatibility with the unique benefits of your chosen architecture.

Sources:
	•	Bruno Scheufler – “Running Untrusted Workloads with Firecracker and containerd” (on integrating Firecracker microVMs as a containerd runtime) ￼ ￼
	•	containerd Discussion #5525 – “HostProcess containers for macOS” (notes on building a containerd shim and snapshotter for macOS containers) ￼
	•	Rancher Desktop Docs – Working with Containers (nerdctl vs Docker CLI and Compose) ￼ ￼
	•	Nerdctl README – (Docker-compatible CLI with Compose support) ￼ ￼
	•	Finch Docs – Finch Architecture (macOS) ￼ ￼
	•	Red Hat Blog – “How Podman runs on Macs” (Podman machine architecture with gvproxy for networking) ￼
	•	AWS re:Invent 2019 – “Deep Dive into firecracker-containerd” (architecture and design decisions for VM-based containers) ￼ ￼

@/vmm 